{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modern',\n",
       " 'generative',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'based',\n",
       " 'mainly',\n",
       " 'on',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'technique',\n",
       " ',',\n",
       " 'therefore',\n",
       " 'generative',\n",
       " 'AI',\n",
       " 'also',\n",
       " 'started',\n",
       " 'rapidly',\n",
       " 'developing',\n",
       " 'in',\n",
       " 'the',\n",
       " '2010s',\n",
       " '.',\n",
       " 'Deep',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'a',\n",
       " 'type',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'that',\n",
       " 'employs',\n",
       " 'multi-layered',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'that',\n",
       " 'self-train',\n",
       " 'on',\n",
       " 'a',\n",
       " 'large',\n",
       " 'dataset',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample string\n",
    "genAI =  'Modern generative AI is based mainly on deep learning technique, therefore generative AI also started rapidly developing in the 2010s. Deep learning is a type of machine learning that employs multi-layered neural networks that self-train on a large dataset.'\n",
    "\n",
    "# tokenizing our sample string\n",
    "genAItokenize = word_tokenize(genAI)\n",
    "genAItokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'learning': 3, 'generative': 2, 'AI': 2, 'is': 2, 'on': 2, '.': 2, 'a': 2, 'that': 2, 'Modern': 1, 'based': 1, ...})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check frequency of tokens\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq_dist = FreqDist()\n",
    "\n",
    "for _ in genAItokenize:\n",
    "    freq_dist[_] = freq_dist[_] + 1\n",
    "\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learning', 3), ('generative', 2), ('AI', 2), ('is', 2), ('on', 2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve most common tokens\n",
    "five_most_occuring_word = freq_dist.most_common(5)\n",
    "five_most_occuring_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Modern', 'generative', 'AI'),\n",
       " ('generative', 'AI', 'is'),\n",
       " ('AI', 'is', 'based'),\n",
       " ('is', 'based', 'mainly'),\n",
       " ('based', 'mainly', 'on'),\n",
       " ('mainly', 'on', 'deep'),\n",
       " ('on', 'deep', 'learning'),\n",
       " ('deep', 'learning', 'technique'),\n",
       " ('learning', 'technique', ','),\n",
       " ('technique', ',', 'therefore'),\n",
       " (',', 'therefore', 'generative'),\n",
       " ('therefore', 'generative', 'AI'),\n",
       " ('generative', 'AI', 'also'),\n",
       " ('AI', 'also', 'started'),\n",
       " ('also', 'started', 'rapidly'),\n",
       " ('started', 'rapidly', 'developing'),\n",
       " ('rapidly', 'developing', 'in'),\n",
       " ('developing', 'in', 'the'),\n",
       " ('in', 'the', '2010s'),\n",
       " ('the', '2010s', '.'),\n",
       " ('2010s', '.', 'Deep'),\n",
       " ('.', 'Deep', 'learning'),\n",
       " ('Deep', 'learning', 'is'),\n",
       " ('learning', 'is', 'a'),\n",
       " ('is', 'a', 'type'),\n",
       " ('a', 'type', 'of'),\n",
       " ('type', 'of', 'machine'),\n",
       " ('of', 'machine', 'learning'),\n",
       " ('machine', 'learning', 'that'),\n",
       " ('learning', 'that', 'employs'),\n",
       " ('that', 'employs', 'multi-layered'),\n",
       " ('employs', 'multi-layered', 'neural'),\n",
       " ('multi-layered', 'neural', 'networks'),\n",
       " ('neural', 'networks', 'that'),\n",
       " ('networks', 'that', 'self-train'),\n",
       " ('that', 'self-train', 'on'),\n",
       " ('self-train', 'on', 'a'),\n",
       " ('on', 'a', 'large'),\n",
       " ('a', 'large', 'dataset'),\n",
       " ('large', 'dataset', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain n combination of tokens - basic driver for contextualization.\n",
    "grouped_tokens = list(nltk.ngrams(genAItokenize, 3))\n",
    "grouped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming - a process to obtain the base form of any word \n",
    "from nltk.stem import  PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'administ'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "portstem = PorterStemmer()\n",
    "\n",
    "result = portstem.stem('administering')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization -  a process of reducing word to their dictionary form. A more enhanced version to stemming\n",
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 'cat', 'cacti': 'cactus'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_stem = ['cats', 'cacti']\n",
    "lemma =  WordNetLemmatizer()\n",
    "\n",
    "result = {data: lemma.lemmatize(data) for data in word_to_stem}\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('why', 'WRB'),\n",
       " ('do', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('bother', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('eat', 'VB'),\n",
       " ('Adel', 'NNP'),\n",
       " ('if', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('hungry', 'JJ')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parts of speech tagging with nltk\n",
    "statement =  'why do you bother to eat Adel if you are not hungry'\n",
    "\n",
    "# firstly create a work token\n",
    "st_token =  word_tokenize(statement)\n",
    "\n",
    "# Pass the list of tokens and create its part of speech \n",
    "result = nltk.pos_tag(st_token)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,976.0,168.0\" width=\"976px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"4.09836%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"2.04918%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"4.09836%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GSP</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">US</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.14754%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.01639%\" x=\"8.19672%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">president</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.7049%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"17.2131%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">has</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.2623%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.19672%\" x=\"21.3115%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">released</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.4098%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.55738%\" x=\"29.5082%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">budget</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"32.7869%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.27869%\" x=\"36.0656%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.7049%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.91803%\" x=\"39.3443%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">over</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.8033%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.27869%\" x=\"44.2623%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">2</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.9016%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.37705%\" x=\"47.541%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">million</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.2295%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.37705%\" x=\"54.918%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">dollars</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.6066%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"62.2951%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">for</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"64.3443%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.4754%\" x=\"66.3934%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">AI</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.1311%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.01639%\" x=\"77.8689%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">invention</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"82.377%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.09836%\" x=\"86.8852%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"88.9344%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.01639%\" x=\"90.9836%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">discovery</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.4918%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('The', 'DT'), Tree('GSP', [('US', 'NNP')]), ('president', 'NN'), ('has', 'VBZ'), ('released', 'VBN'), ('budget', 'NN'), ('of', 'IN'), ('over', 'IN'), ('2', 'CD'), ('million', 'CD'), ('dollars', 'NNS'), ('for', 'IN'), Tree('ORGANIZATION', [('AI', 'NNP')]), ('invention', 'NN'), ('and', 'CC'), ('discovery', 'NN')])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import svgling # this module has a function that can draw the tree for the NE relationship\n",
    "\n",
    "# named entity recognition\n",
    "statement = 'The US president has released budget of over 2 million dollars for AI invention and discovery'\n",
    "\n",
    "# firstly obtain its token\n",
    "st_token =  word_tokenize(statement)\n",
    "\n",
    "# secondly, obtain its part of speech for each word\n",
    "result = nltk.pos_tag(st_token)\n",
    "\n",
    "# thirdly, invoke the named entity module and pass the tagged data\n",
    "final_output = nltk.ne_chunk(result)\n",
    "final_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Spacy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the package\n",
    "nlp =  spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The DET\n",
      "1 US PROPN\n",
      "2 president NOUN\n",
      "3 has AUX\n",
      "4 released VERB\n",
      "5 budget NOUN\n",
      "6 of ADP\n",
      "7 over ADP\n",
      "8 2 NUM\n",
      "9 million NUM\n",
      "10 dollars NOUN\n",
      "11 for ADP\n",
      "12 AI PROPN\n",
      "13 invention NOUN\n",
      "14 and CCONJ\n",
      "15 discovery NOUN\n"
     ]
    }
   ],
   "source": [
    "statement = 'The US president has released budget of over 2 million dollars for AI invention and discovery'\n",
    "spacy_data = nlp(statement)\n",
    "\n",
    "\n",
    "for data in spacy_data:\n",
    "    print(data.i, data.text, data.pos_) # print each value index position, text and part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US GPE\n",
      "over 2 million dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "for data in spacy_data.ents: # obtaining the named entity for each item on the doc\n",
    "    print(data.text, data.label_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matcher\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using matcher to extract any substring from a string\n",
    "statement = 'The US president has released budget of over 2 million dollars for AI invention and discovery'\n",
    "\n",
    "# create a doc \n",
    "doc = nlp(statement)\n",
    "\n",
    "# invoke the matcher class\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# pattern we are interested in extracting\n",
    "pattern =  [{\"ORTH\" : \"president\"}, {\"LOWER\": \"invention\"}, {\"IS_DIGIT\": True}, {\"ENT_TYPE\": \"2 million dollars\"}]\n",
    "\n",
    "matcher.add('AI', [pattern])\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to obtain the matched items\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import re\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from nltk.classify import ClassifierI\n",
    "from sklearn.svm import SVC\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents - the reviews( both positive and negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first task - create a list of tuples (where the first element is the review and the second is the label - \"positive\")\n",
    "\n",
    "# second tasks - remove punctuations (optional)\n",
    "\n",
    "# third task - remove all stop words\n",
    "# we will need the package\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "\n",
    "# create a list of stop words from a particular language(English)\n",
    "stopwords_list = list(set(stopwords.words('english')))\n",
    "\n",
    "# fourth task - part of speech tagging\n",
    "\n",
    "# fifth task - make a list of all the adjectives identified"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
